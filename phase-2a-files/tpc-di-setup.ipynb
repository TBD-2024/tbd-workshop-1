{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a0ed7a-a23a-4523-9171-f1d076700168",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_BUCKET=tbd-2024z-336369-data\n",
      "env: GEN_OUTPUT_DIR=/tmp/tpc-di\n",
      "env: REPO_ROOT=/home/jupyter/git/tbd-tpc-di/\n"
     ]
    }
   ],
   "source": [
    "%env DATA_BUCKET=tbd-2024z-336369-data\n",
    "%env GEN_OUTPUT_DIR=/tmp/tpc-di\n",
    "%env REPO_ROOT=/home/jupyter/git/tbd-tpc-di/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fbec0d-d410-4de9-8dfd-00b07fdb7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typer==0.9.0 (from typer[All]==0.9.0)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-cloud-storage==2.13.0\n",
      "  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (8.1.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (4.12.2)\n",
      "Collecting google-auth<3.0dev,>=2.23.3 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.32.3)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (0.4.6)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[All]==0.9.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[All]==0.9.0)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0) (4.25.5)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2019.11.28)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[All]==0.9.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: typer, shellingham, pyasn1, proto-plus, mdurl, googleapis-common-protos, google-crc32c, cachetools, rsa, pyasn1-modules, markdown-it-py, google-resumable-media, rich, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-5.5.0 google-api-core-2.24.0 google-auth-2.37.0 google-cloud-core-2.4.1 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.7.2 googleapis-common-protos-1.66.0 markdown-it-py-3.0.0 mdurl-0.1.2 proto-plus-1.25.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rich-13.9.4 rsa-4.9 shellingham-1.5.4 typer-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3.8 install typer[All]==0.9.0 google-cloud-storage==2.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b5820-852d-4c30-a178-9dd78bbdbd5f",
   "metadata": {},
   "source": [
    "## Install SDKMAN for setting up JVM 8 enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3cdb5b-4978-479e-85c7-c9dbac3e65fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                -+syyyyyyys:\n",
      "                            `/yho:`       -yd.\n",
      "                         `/yh/`             +m.\n",
      "                       .oho.                 hy                          .`\n",
      "                     .sh/`                   :N`                `-/o`  `+dyyo:.\n",
      "                   .yh:`                     `M-          `-/osysoym  :hs` `-+sys:      hhyssssssssy+\n",
      "                 .sh:`                       `N:          ms/-``  yy.yh-      -hy.    `.N-````````+N.\n",
      "               `od/`                         `N-       -/oM-      ddd+`     `sd:     hNNm        -N:\n",
      "              :do`                           .M.       dMMM-     `ms.      /d+`     `NMMs       `do\n",
      "            .yy-                             :N`    ```mMMM.      -      -hy.       /MMM:       yh\n",
      "          `+d+`           `:/oo/`       `-/osyh/ossssssdNMM`           .sh:         yMMN`      /m.\n",
      "         -dh-           :ymNMMMMy  `-/shmNm-`:N/-.``   `.sN            /N-         `NMMy      .m/\n",
      "       `oNs`          -hysosmMMMMydmNmds+-.:ohm           :             sd`        :MMM/      yy\n",
      "      .hN+           /d:    -MMMmhs/-.`   .MMMh   .ss+-                 `yy`       sMMN`     :N.\n",
      "     :mN/           `N/     `o/-`         :MMMo   +MMMN-         .`      `ds       mMMh      do\n",
      "    /NN/            `N+....--:/+oooosooo+:sMMM:   hMMMM:        `my       .m+     -MMM+     :N.\n",
      "   /NMo              -+ooooo+/:-....`...:+hNMN.  `NMMMd`        .MM/       -m:    oMMN.     hs\n",
      "  -NMd`                                    :mm   -MMMm- .s/     -MMm.       /m-   mMMd     -N.\n",
      " `mMM/                                      .-   /MMh. -dMo     -MMMy        od. .MMMs..---yh\n",
      " +MMM.                                           sNo`.sNMM+     :MMMM/        sh`+MMMNmNm+++-\n",
      " mMMM-                                           /--ohmMMM+     :MMMMm.       `hyymmmdddo\n",
      " MMMMh.                  ````                  `-+yy/`yMMM/     :MMMMMy       -sm:.``..-:-.`\n",
      " dMMMMmo-.``````..-:/osyhddddho.           `+shdh+.   hMMM:     :MmMMMM/   ./yy/` `:sys+/+sh/\n",
      " .dMMMMMMmdddddmmNMMMNNNNNMMMMMs           sNdo-      dMMM-  `-/yd/MMMMm-:sy+.   :hs-      /N`\n",
      "  `/ymNNNNNNNmmdys+/::----/dMMm:          +m-         mMMM+ohmo/.` sMMMMdo-    .om:       `sh\n",
      "     `.-----+/.`       `.-+hh/`         `od.          NMMNmds/     `mmy:`     +mMy      `:yy.\n",
      "           /moyso+//+ossso:.           .yy`          `dy+:`         ..       :MMMN+---/oys:\n",
      "         /+m:  `.-:::-`               /d+                                    +MMMMMMMNh:`\n",
      "        +MN/                        -yh.                                     `+hddhy+.\n",
      "       /MM+                       .sh:\n",
      "      :NMo                      -sh/\n",
      "     -NMs                    `/yy:\n",
      "    .NMy                  `:sh+.\n",
      "   `mMm`               ./yds-\n",
      "  `dMMMmyo:-.````.-:oymNy:`\n",
      "  +NMMMMMMMMMMMMMMMMms:`\n",
      "    -+shmNMMMNmdy+:`\n",
      "\n",
      "\n",
      "                                                                 Now attempting installation...\n",
      "\n",
      "\n",
      "Looking for a previous installation of SDKMAN...\n",
      "Looking for unzip...\n",
      "Looking for zip...\n",
      "Looking for tar...\n",
      "Looking for curl...\n",
      "Looking for sed...\n",
      "Installing SDKMAN scripts...\n",
      "Create distribution directories...\n",
      "Getting available candidates...\n",
      "Prime platform file...\n",
      "Prime the config file...\n",
      "Installing script cli archive...\n",
      "* Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Checking archive integrity...\n",
      "* Extracting archive...\n",
      "* Copying archive contents...\n",
      "* Cleaning up...\n",
      "\n",
      "Installing script cli archive...\n",
      "* Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Checking archive integrity...\n",
      "* Extracting archive...\n",
      "* Copying archive contents...\n",
      "* Cleaning up...\n",
      "\n",
      "Set version to 5.18.2 ...\n",
      "Set native version to 0.5.0 ...\n",
      "Attempt update of interactive bash profile on regular UNIX...\n",
      "Added sdkman init snippet to /root/.bashrc\n",
      "Attempt update of zsh profile...\n",
      "Updated existing /root/.zshrc\n",
      "\n",
      "\n",
      "\n",
      "All done!\n",
      "\n",
      "\n",
      "You are subscribed to the STABLE channel.\n",
      "\n",
      "Please open a new terminal, or run the following in the existing one:\n",
      "\n",
      "    source \"/root/.sdkman/bin/sdkman-init.sh\"\n",
      "\n",
      "Then issue the following command:\n",
      "\n",
      "    sdk help\n",
      "\n",
      "Enjoy!!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s https://get.sdkman.io | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691084f8-a883-408a-8d16-cc717669c04d",
   "metadata": {},
   "source": [
    "## Install and set as default JVM 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275488e6-68e3-4f42-a3ca-060b286bade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: java 8.0.392-amzn\n",
      "\n",
      "In progress...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repackaging Java 8.0.392-amzn...\n",
      "\n",
      "Done repackaging...\n",
      "\n",
      "\u001b[1;32mInstalling: java 8.0.392-amzn\u001b[0m\n",
      "\u001b[1;32mDone installing!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;32mSetting java 8.0.392-amzn as default.\u001b[0m\n",
      "\n",
      "\u001b[1;32mUsing java version 8.0.392-amzn in this shell.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 8.0.392-amzn\n",
    "sdk use java 8.0.392-amzn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554db58-b832-45e2-a1f6-28bd873ae31d",
   "metadata": {},
   "source": [
    "## Check if JVM 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "898835b7-c347-49dd-bc0f-ca845375e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment Corretto-8.392.08.1 (build 1.8.0_392-b08)\n",
      "OpenJDK 64-Bit Server VM Corretto-8.392.08.1 (build 25.392-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45d9a3-e599-4661-944b-1bdfe3808c19",
   "metadata": {},
   "source": [
    "## Clone tbd-tpc-di repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7481d445-4ea6-40cc-8320-5520fe8d4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tbd-tpc-di'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'notebook'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'notebook' set up to track remote branch 'notebook' from 'origin'.\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p git && cd git\n",
    "git clone https://github.com/TBD-2024/tbd-tpc-di\n",
    "cd tbd-tpc-di\n",
    "git pull\n",
    "git checkout notebook\n",
    "git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b992bfc-8f8c-42eb-bf07-069b0d897fda",
   "metadata": {},
   "source": [
    "## Generate input dataset (run this cell below from the terminal!!!)\n",
    "It should take approx. 15min with scale factor set to 100 and generate approx. 10GiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ab901-15dc-4e94-b4fd-e2b9cfc6e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd /home/jupyter/git/tbd-tpc-di/tools/ \n",
    "java -jar DIGen.jar -sf 100 -o /tmp/tpc-di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24c0aa-bff7-4831-869d-3183e7373a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup JVM 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62e466e8-41aa-4afc-8392-5675d36adcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;33mjava 11.0.21-amzn is already installed.\u001b[0m\n",
      "\n",
      "\u001b[1;32mUsing java version 11.0.21-amzn in this shell.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 11.0.21-amzn\n",
    "sdk use java 11.0.21-amzn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccda4c-40d4-40fe-9d2e-68b416522f64",
   "metadata": {},
   "source": [
    "## Load staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "259b72bc-2efd-405b-883f-618a9772bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6f0967e1-f18e-4656-96a1-58fc906c38d1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 718ms :: artifacts dl 46ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6f0967e1-f18e-4656-96a1-58fc906c38d1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/30ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 11:57:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 11:57:30 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/12/22 11:57:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/12/22 11:57:44 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/12/22 11:57:44 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/12/22 11:57:44 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/12/22 11:57:44 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/12/22 11:57:44 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/12/22 11:58:12 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "24/12/22 12:03:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "DATE table created.\n",
      "DAILY_MARKET table created.\n",
      "INDUSTRY table created.\n",
      "PROSPECT table created.\n",
      "CUSTOMER_MGMT table created.\n",
      "TAX_RATE table created.\n",
      "HR table created.\n",
      "WATCH_HISTORY table created.\n",
      "TRADE table created.\n",
      "TRADE_HISTORY table created.\n",
      "STATUS_TYPE table created.\n",
      "TRADE_TYPE table created.\n",
      "HOLDING_HISTORY table created.\n",
      "CASH_TRANSACTION table created.\n",
      "CMP table created.\n",
      "SEC table created.\n",
      "FIN table created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd $REPO_ROOT\n",
    "python3.8 tpcdi.py --output-directory $GEN_OUTPUT_DIR --stage $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0740193-5da4-4aac-9349-aa4e76f4e99b",
   "metadata": {},
   "source": [
    "## Run dbt ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d0642c-b5c9-4aa8-996f-40f82e3c90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m12:30:01  Running with dbt=1.7.13\n",
      "\u001b[0m12:30:02  Installing dbt-labs/dbt_utils\n",
      "\u001b[0m12:30:02  Installed from version 1.1.1\n",
      "\u001b[0m12:30:02  Updated version available: 1.3.0\n",
      "\u001b[0m12:30:02  \n",
      "\u001b[0m12:30:02  Updates available for packages: ['dbt-labs/dbt_utils']                 \n",
      "Update your versions in packages.yml, then run dbt deps\n",
      "\u001b[0m12:30:13  Running with dbt=1.7.13\n",
      "\u001b[0m12:30:14  Registered adapter: spark=1.7.1\n",
      "\u001b[0m12:30:15  Found 44 models, 1 test, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m12:30:15  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fc8929de-4641-40c9-871b-aab5d4e44162;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 1172ms :: artifacts dl 66ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fc8929de-4641-40c9-871b-aab5d4e44162\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/46ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 12:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 12:30:32 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/12/22 12:30:33 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/12/22 12:30:54 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/12/22 12:30:54 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/12/22 12:30:54 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/12/22 12:30:54 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/12/22 12:30:54 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/12/22 12:31:15 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m12:31:18  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m12:31:18  \n",
      "\u001b[0m12:31:18  1 of 43 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]\n",
      "24/12/22 12:31:20 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/22 12:31:21 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m12:32:14  1 of 43 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [\u001b[32mOK\u001b[0m in 55.20s]\n",
      "\u001b[0m12:32:14  2 of 43 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]\n",
      "24/12/22 12:32:14 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:34:01  2 of 43 OK created sql table model demo_bronze.brokerage_daily_market .......... [\u001b[32mOK\u001b[0m in 107.66s]\n",
      "\u001b[0m12:34:01  3 of 43 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]\n",
      "24/12/22 12:34:02 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:34:13  3 of 43 OK created sql table model demo_bronze.brokerage_holding_history ....... [\u001b[32mOK\u001b[0m in 11.49s]\n",
      "\u001b[0m12:34:13  4 of 43 START sql table model demo_bronze.brokerage_trade ...................... [RUN]\n",
      "24/12/22 12:34:13 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:35:35  4 of 43 OK created sql table model demo_bronze.brokerage_trade ................. [\u001b[32mOK\u001b[0m in 81.93s]\n",
      "\u001b[0m12:35:35  5 of 43 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]\n",
      "24/12/22 12:35:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:36:31  5 of 43 OK created sql table model demo_bronze.brokerage_trade_history ......... [\u001b[32mOK\u001b[0m in 56.00s]\n",
      "\u001b[0m12:36:31  6 of 43 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]\n",
      "24/12/22 12:36:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:37:26  6 of 43 OK created sql table model demo_bronze.brokerage_watch_history ......... [\u001b[32mOK\u001b[0m in 54.82s]\n",
      "\u001b[0m12:37:26  7 of 43 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]\n",
      "24/12/22 12:37:26 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/22 12:37:26 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\u001b[0m12:37:35  7 of 43 OK created sql table model demo_bronze.crm_customer_mgmt ............... [\u001b[32mOK\u001b[0m in 9.63s]\n",
      "\u001b[0m12:37:35  8 of 43 START sql table model demo_bronze.finwire_company ...................... [RUN]\n",
      "24/12/22 12:37:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:37:38  8 of 43 OK created sql table model demo_bronze.finwire_company ................. [\u001b[32mOK\u001b[0m in 2.42s]\n",
      "\u001b[0m12:37:38  9 of 43 START sql table model demo_bronze.finwire_financial .................... [RUN]\n",
      "24/12/22 12:37:38 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:38:57  9 of 43 OK created sql table model demo_bronze.finwire_financial ............... [\u001b[32mOK\u001b[0m in 79.49s]\n",
      "\u001b[0m12:38:57  10 of 43 START sql table model demo_bronze.finwire_security .................... [RUN]\n",
      "24/12/22 12:38:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:00  10 of 43 OK created sql table model demo_bronze.finwire_security ............... [\u001b[32mOK\u001b[0m in 3.11s]\n",
      "\u001b[0m12:39:00  11 of 43 START sql table model demo_bronze.hr_employee ......................... [RUN]\n",
      "24/12/22 12:39:01 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:03  11 of 43 OK created sql table model demo_bronze.hr_employee .................... [\u001b[32mOK\u001b[0m in 2.51s]\n",
      "\u001b[0m12:39:03  12 of 43 START sql table model demo_bronze.reference_date ...................... [RUN]\n",
      "24/12/22 12:39:03 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:05  12 of 43 OK created sql table model demo_bronze.reference_date ................. [\u001b[32mOK\u001b[0m in 1.77s]\n",
      "\u001b[0m12:39:05  13 of 43 START sql table model demo_bronze.reference_industry .................. [RUN]\n",
      "24/12/22 12:39:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:06  13 of 43 OK created sql table model demo_bronze.reference_industry ............. [\u001b[32mOK\u001b[0m in 1.08s]\n",
      "\u001b[0m12:39:06  14 of 43 START sql table model demo_bronze.reference_status_type ............... [RUN]\n",
      "24/12/22 12:39:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:07  14 of 43 OK created sql table model demo_bronze.reference_status_type .......... [\u001b[32mOK\u001b[0m in 1.08s]\n",
      "\u001b[0m12:39:07  15 of 43 START sql table model demo_bronze.reference_tax_rate .................. [RUN]\n",
      "24/12/22 12:39:07 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:08  15 of 43 OK created sql table model demo_bronze.reference_tax_rate ............. [\u001b[32mOK\u001b[0m in 1.14s]\n",
      "\u001b[0m12:39:08  16 of 43 START sql table model demo_bronze.reference_trade_type ................ [RUN]\n",
      "24/12/22 12:39:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:09  16 of 43 OK created sql table model demo_bronze.reference_trade_type ........... [\u001b[32mOK\u001b[0m in 1.03s]\n",
      "\u001b[0m12:39:09  17 of 43 START sql table model demo_bronze.syndicated_prospect ................. [RUN]\n",
      "24/12/22 12:39:09 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m12:39:15  17 of 43 OK created sql table model demo_bronze.syndicated_prospect ............ [\u001b[32mOK\u001b[0m in 6.14s]\n",
      "\u001b[0m12:39:15  18 of 43 START sql table model demo_silver.daily_market ........................ [RUN]\n",
      "24/12/22 12:39:16 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:13:28  18 of 43 OK created sql table model demo_silver.daily_market ................... [\u001b[32mOK\u001b[0m in 2052.35s]\n",
      "\u001b[0m13:13:28  19 of 43 START sql table model demo_silver.employees ........................... [RUN]\n",
      "24/12/22 13:13:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:13:31  19 of 43 OK created sql table model demo_silver.employees ...................... [\u001b[32mOK\u001b[0m in 3.77s]\n",
      "\u001b[0m13:13:31  20 of 43 START sql table model demo_silver.date ................................ [RUN]\n",
      "24/12/22 13:13:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:13:33  20 of 43 OK created sql table model demo_silver.date ........................... [\u001b[32mOK\u001b[0m in 1.38s]\n",
      "\u001b[0m13:13:33  21 of 43 START sql table model demo_silver.companies ........................... [RUN]\n",
      "24/12/22 13:13:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:13:39  21 of 43 OK created sql table model demo_silver.companies ...................... [\u001b[32mOK\u001b[0m in 6.53s]\n",
      "\u001b[0m13:13:39  22 of 43 START sql table model demo_silver.accounts ............................ [RUN]\n",
      "24/12/22 13:13:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:13:56  22 of 43 OK created sql table model demo_silver.accounts ....................... [\u001b[32mOK\u001b[0m in 17.07s]\n",
      "\u001b[0m13:13:56  23 of 43 START sql table model demo_silver.customers ........................... [RUN]\n",
      "24/12/22 13:13:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:14:13  23 of 43 OK created sql table model demo_silver.customers ...................... [\u001b[32mOK\u001b[0m in 15.39s]\n",
      "\u001b[0m13:14:13  24 of 43 START sql table model demo_silver.trades_history ...................... [RUN]\n",
      "24/12/22 13:14:13 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:23:30  24 of 43 OK created sql table model demo_silver.trades_history ................. [\u001b[32mOK\u001b[0m in 557.30s]\n",
      "\u001b[0m13:23:30  25 of 43 START sql table model demo_gold.dim_broker ............................ [RUN]\n",
      "24/12/22 13:23:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:23:35  25 of 43 OK created sql table model demo_gold.dim_broker ....................... [\u001b[32mOK\u001b[0m in 4.46s]\n",
      "\u001b[0m13:23:35  26 of 43 START sql table model demo_gold.dim_date .............................. [RUN]\n",
      "24/12/22 13:23:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:23:36  26 of 43 OK created sql table model demo_gold.dim_date ......................... [\u001b[32mOK\u001b[0m in 1.62s]\n",
      "\u001b[0m13:23:36  27 of 43 START sql table model demo_gold.dim_company ........................... [RUN]\n",
      "24/12/22 13:23:37 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:23:39  27 of 43 OK created sql table model demo_gold.dim_company ...................... [\u001b[32mOK\u001b[0m in 3.01s]\n",
      "\u001b[0m13:23:39  28 of 43 START sql table model demo_silver.financials .......................... [RUN]\n",
      "24/12/22 13:23:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:25:34  28 of 43 OK created sql table model demo_silver.financials ..................... [\u001b[32mOK\u001b[0m in 114.84s]\n",
      "\u001b[0m13:25:34  29 of 43 START sql table model demo_silver.securities .......................... [RUN]\n",
      "24/12/22 13:25:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:25:41  29 of 43 OK created sql table model demo_silver.securities ..................... [\u001b[32mOK\u001b[0m in 6.83s]\n",
      "\u001b[0m13:25:41  30 of 43 START sql table model demo_silver.cash_transactions ................... [RUN]\n",
      "24/12/22 13:25:41 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:27:00  30 of 43 OK created sql table model demo_silver.cash_transactions .............. [\u001b[32mOK\u001b[0m in 79.33s]\n",
      "\u001b[0m13:27:00  31 of 43 START sql table model demo_gold.dim_customer .......................... [RUN]\n",
      "24/12/22 13:27:01 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:27:27  31 of 43 OK created sql table model demo_gold.dim_customer ..................... [\u001b[32mOK\u001b[0m in 26.28s]\n",
      "\u001b[0m13:27:27  32 of 43 START sql table model demo_gold.dim_trade ............................. [RUN]\n",
      "24/12/22 13:27:27 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:33:09  32 of 43 OK created sql table model demo_gold.dim_trade ........................ [\u001b[32mOK\u001b[0m in 342.37s]\n",
      "\u001b[0m13:33:09  33 of 43 START sql table model demo_silver.trades .............................. [RUN]\n",
      "24/12/22 13:33:09 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:40:11  33 of 43 OK created sql table model demo_silver.trades ......................... [\u001b[32mOK\u001b[0m in 421.69s]\n",
      "\u001b[0m13:40:11  34 of 43 START sql table model demo_gold.dim_security .......................... [RUN]\n",
      "24/12/22 13:40:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:40:15  34 of 43 OK created sql table model demo_gold.dim_security ..................... [\u001b[32mOK\u001b[0m in 3.72s]\n",
      "\u001b[0m13:40:15  35 of 43 START sql table model demo_silver.watches_history ..................... [RUN]\n",
      "24/12/22 13:40:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:43:25  35 of 43 OK created sql table model demo_silver.watches_history ................ [\u001b[32mOK\u001b[0m in 190.14s]\n",
      "\u001b[0m13:43:25  36 of 43 START sql table model demo_gold.dim_account ........................... [RUN]\n",
      "24/12/22 13:43:25 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:43:35  36 of 43 OK created sql table model demo_gold.dim_account ...................... [\u001b[32mOK\u001b[0m in 10.31s]\n",
      "\u001b[0m13:43:35  37 of 43 START sql table model demo_silver.holdings_history .................... [RUN]\n",
      "24/12/22 13:43:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:46:18  37 of 43 OK created sql table model demo_silver.holdings_history ............... [\u001b[32mOK\u001b[0m in 163.23s]\n",
      "\u001b[0m13:46:18  38 of 43 START sql table model demo_silver.watches ............................. [RUN]\n",
      "24/12/22 13:46:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:50:48  38 of 43 OK created sql table model demo_silver.watches ........................ [\u001b[32mOK\u001b[0m in 269.33s]\n",
      "\u001b[0m13:50:48  39 of 43 START sql table model demo_gold.fact_cash_transactions ................ [RUN]\n",
      "24/12/22 13:50:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:52:12  39 of 43 OK created sql table model demo_gold.fact_cash_transactions ........... [\u001b[32mOK\u001b[0m in 84.49s]\n",
      "\u001b[0m13:52:12  40 of 43 START sql table model demo_gold.fact_trade ............................ [RUN]\n",
      "24/12/22 13:52:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:56:55  40 of 43 OK created sql table model demo_gold.fact_trade ....................... [\u001b[32mOK\u001b[0m in 282.58s]\n",
      "\u001b[0m13:56:55  41 of 43 START sql table model demo_gold.fact_holdings ......................... [RUN]\n",
      "24/12/22 13:56:55 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:18:23  41 of 43 OK created sql table model demo_gold.fact_holdings .................... [\u001b[32mOK\u001b[0m in 1288.44s]\n",
      "\u001b[0m14:18:23  42 of 43 START sql table model demo_gold.fact_watches .......................... [RUN]\n",
      "24/12/22 14:18:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:21:49  42 of 43 OK created sql table model demo_gold.fact_watches ..................... [\u001b[32mOK\u001b[0m in 205.78s]\n",
      "\u001b[0m14:21:49  43 of 43 START sql table model demo_gold.fact_cash_balances .................... [RUN]\n",
      "24/12/22 14:21:49 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:28:24  43 of 43 OK created sql table model demo_gold.fact_cash_balances ............... [\u001b[32mOK\u001b[0m in 394.96s]\n",
      "\u001b[0m14:28:24  \n",
      "\u001b[0m14:28:24  Finished running 43 table models in 1 hours 58 minutes and 9.42 seconds (7089.42s).\n",
      "\u001b[0m14:28:24  \n",
      "\u001b[0m14:28:24  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m14:28:24  \n",
      "\u001b[0m14:28:24  Done. PASS=43 WARN=0 ERROR=0 SKIP=0 TOTAL=43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-154' coro=<ScriptMagics.shebang.<locals>._handle_stream() done, defined at /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:211> exception=ValueError('Separator is not found, and chunk exceed the limit')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 540, in readline\n",
      "    line = await self.readuntil(sep)\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 618, in readuntil\n",
      "    raise exceptions.LimitOverrunError(\n",
      "asyncio.exceptions.LimitOverrunError: Separator is not found, and chunk exceed the limit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\", line 213, in _handle_stream\n",
      "    line = (await stream.readline()).decode(\"utf8\", errors=\"replace\")\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 549, in readline\n",
      "    raise ValueError(e.args[0])\n",
      "ValueError: Separator is not found, and chunk exceed the limit\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt deps\n",
    "dbt run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae93304-68eb-46c8-86a3-26feec06a54a",
   "metadata": {},
   "source": [
    "## Run dbt tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6b563f4-0b31-40a3-b353-9e6c9de8c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m14:32:04  Running with dbt=1.7.13\n",
      "\u001b[0m14:32:06  Registered adapter: spark=1.7.1\n",
      "\u001b[0m14:32:07  Found 44 models, 1 test, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m14:32:07  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-962127b8-7547-4a2a-bd1c-bd1dcd385129;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 786ms :: artifacts dl 38ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-962127b8-7547-4a2a-bd1c-bd1dcd385129\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/28ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 14:32:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 14:32:26 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/12/22 14:32:26 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/12/22 14:32:40 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:32:40 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:32:40 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/12/22 14:32:40 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:32:40 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:32:59 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m14:33:03  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m14:33:03  \n",
      "\u001b[0m14:33:03  1 of 1 START test fact_trade__unique_trade ..................................... [RUN]\n",
      "24/12/22 14:33:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m14:33:42  1 of 1 PASS fact_trade__unique_trade ........................................... [\u001b[32mPASS\u001b[0m in 38.84s]\n",
      "\u001b[0m14:33:42  \n",
      "\u001b[0m14:33:42  Finished running 1 test in 0 hours 1 minutes and 34.86 seconds (94.86s).\n",
      "\u001b[0m14:33:42  \n",
      "\u001b[0m14:33:42  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m14:33:42  \n",
      "\u001b[0m14:33:42  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6ce00fe-ce46-4202-a798-a7f227ff3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-29c509af-4f7f-46ba-b0aa-19f0643ec0f1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 865ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-29c509af-4f7f-46ba-b0aa-19f0643ec0f1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/13ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 14:36:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 14:36:14 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/12/22 14:36:15 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/12/22 14:36:22 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:36:22 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:36:22 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/12/22 14:36:22 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/12/22 14:36:22 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TBD-TPC-DI-setup\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"256m\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c892605-9496-4526-b574-a19168678934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/22 14:38:15 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|     bronze|\n",
      "|    default|\n",
      "|demo_bronze|\n",
      "|  demo_gold|\n",
      "|demo_silver|\n",
      "|      digen|\n",
      "|       gold|\n",
      "|     silver|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2e94c83-4983-49f5-8597-77e3b9c4661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use demo_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44296edd-82e6-4600-b730-d2cc4992a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|demo_gold|         dim_account|      false|\n",
      "|demo_gold|          dim_broker|      false|\n",
      "|demo_gold|         dim_company|      false|\n",
      "|demo_gold|        dim_customer|      false|\n",
      "|demo_gold|            dim_date|      false|\n",
      "|demo_gold|        dim_security|      false|\n",
      "|demo_gold|           dim_trade|      false|\n",
      "|demo_gold|  fact_cash_balances|      false|\n",
      "|demo_gold|fact_cash_transac...|      false|\n",
      "|demo_gold|       fact_holdings|      false|\n",
      "|demo_gold|          fact_trade|      false|\n",
      "|demo_gold|        fact_watches|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e34fa44-084c-4445-ad41-17b4762a0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190b0c0-b4e2-4aed-85aa-1b78b77d1c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
