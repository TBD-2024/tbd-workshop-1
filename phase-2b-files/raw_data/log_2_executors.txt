21:59:43  Running with dbt=1.7.13
21:59:44  Installing dbt-labs/dbt_utils
21:59:44  Installed from version 1.1.1
21:59:44  Updated version available: 1.3.0
21:59:44  
21:59:44  Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
21:59:48  Running with dbt=1.7.13
21:59:49  Registered adapter: spark=1.7.1
21:59:49  Found 44 models, 1 test, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models
21:59:49  
:: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml

Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
com.databricks#spark-xml_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-40e316da-d48d-4bcc-bbbd-acc13c565af8;1.0
	confs: [default]
	found com.databricks#spark-xml_2.12;0.17.0 in central
	found commons-io#commons-io;2.11.0 in central
	found org.glassfish.jaxb#txw2;3.0.2 in central
	found org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central
	found org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central
:: resolution report :: resolve 599ms :: artifacts dl 41ms
	:: modules in use:
	com.databricks#spark-xml_2.12;0.17.0 from central in [default]
	commons-io#commons-io;2.11.0 from central in [default]
	org.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]
	org.glassfish.jaxb#txw2;3.0.2 from central in [default]
	org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-40e316da-d48d-4bcc-bbbd-acc13c565af8
	confs: [default]
	0 artifacts copied, 5 already retrieved (0kB/22ms)
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release

25/01/24 21:59:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).

25/01/24 22:00:00 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
25/01/24 22:00:01 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
25/01/24 22:00:21 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.
25/01/24 22:00:21 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.
25/01/24 22:00:21 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.
25/01/24 22:00:21 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.
25/01/24 22:00:21 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.
25/01/24 22:01:11 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
22:01:15  Concurrency: 1 threads (target='dev')
22:01:15  
22:01:15  1 of 43 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]
25/01/24 22:01:16 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
25/01/24 22:01:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
22:02:07  1 of 43 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [OK in 51.52s]
22:02:07  2 of 43 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]
25/01/24 22:02:07 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:03:41  2 of 43 OK created sql table model demo_bronze.brokerage_daily_market .......... [OK in 94.71s]
22:03:41  3 of 43 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]
25/01/24 22:03:42 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:03:54  3 of 43 OK created sql table model demo_bronze.brokerage_holding_history ....... [OK in 12.23s]
22:03:54  4 of 43 START sql table model demo_bronze.brokerage_trade ...................... [RUN]
25/01/24 22:03:54 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:04:43  4 of 43 OK created sql table model demo_bronze.brokerage_trade ................. [OK in 49.95s]
22:04:44  5 of 43 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]
25/01/24 22:04:44 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:05:34  5 of 43 OK created sql table model demo_bronze.brokerage_trade_history ......... [OK in 50.17s]
22:05:34  6 of 43 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]
25/01/24 22:05:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:06:48  6 of 43 OK created sql table model demo_bronze.brokerage_watch_history ......... [OK in 73.97s]
22:06:48  7 of 43 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]
25/01/24 22:06:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
25/01/24 22:06:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
22:07:00  7 of 43 OK created sql table model demo_bronze.crm_customer_mgmt ............... [OK in 12.05s]
22:07:00  8 of 43 START sql table model demo_bronze.finwire_company ...................... [RUN]
25/01/24 22:07:00 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:07:04  8 of 43 OK created sql table model demo_bronze.finwire_company ................. [OK in 4.31s]
22:07:04  9 of 43 START sql table model demo_bronze.finwire_financial .................... [RUN]
25/01/24 22:07:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:23  9 of 43 OK created sql table model demo_bronze.finwire_financial ............... [OK in 79.22s]
22:08:23  10 of 43 START sql table model demo_bronze.finwire_security .................... [RUN]
25/01/24 22:08:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:28  10 of 43 OK created sql table model demo_bronze.finwire_security ............... [OK in 4.83s]
22:08:28  11 of 43 START sql table model demo_bronze.hr_employee ......................... [RUN]
25/01/24 22:08:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:33  11 of 43 OK created sql table model demo_bronze.hr_employee .................... [OK in 4.95s]
22:08:33  12 of 43 START sql table model demo_bronze.reference_date ...................... [RUN]
25/01/24 22:08:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:35  12 of 43 OK created sql table model demo_bronze.reference_date ................. [OK in 2.03s]
22:08:35  13 of 43 START sql table model demo_bronze.reference_industry .................. [RUN]
25/01/24 22:08:36 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:36  13 of 43 OK created sql table model demo_bronze.reference_industry ............. [OK in 1.06s]
22:08:36  14 of 43 START sql table model demo_bronze.reference_status_type ............... [RUN]
25/01/24 22:08:37 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:37  14 of 43 OK created sql table model demo_bronze.reference_status_type .......... [OK in 1.06s]
22:08:37  15 of 43 START sql table model demo_bronze.reference_tax_rate .................. [RUN]
25/01/24 22:08:38 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:38  15 of 43 OK created sql table model demo_bronze.reference_tax_rate ............. [OK in 1.05s]
22:08:38  16 of 43 START sql table model demo_bronze.reference_trade_type ................ [RUN]
25/01/24 22:08:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:40  16 of 43 OK created sql table model demo_bronze.reference_trade_type ........... [OK in 1.13s]
22:08:40  17 of 43 START sql table model demo_bronze.syndicated_prospect ................. [RUN]
25/01/24 22:08:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:08:47  17 of 43 OK created sql table model demo_bronze.syndicated_prospect ............ [OK in 7.38s]
22:08:47  18 of 43 START sql table model demo_silver.daily_market ........................ [RUN]
25/01/24 22:08:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:45:07  18 of 43 OK created sql table model demo_silver.daily_market ................... [OK in 2179.91s]
22:45:07  19 of 43 START sql table model demo_silver.employees ........................... [RUN]
25/01/24 22:45:07 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:45:12  19 of 43 OK created sql table model demo_silver.employees ...................... [OK in 5.15s]
22:45:12  20 of 43 START sql table model demo_silver.date ................................ [RUN]
25/01/24 22:45:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:45:14  20 of 43 OK created sql table model demo_silver.date ........................... [OK in 2.31s]
22:45:14  21 of 43 START sql table model demo_silver.companies ........................... [RUN]
25/01/24 22:45:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:45:24  21 of 43 OK created sql table model demo_silver.companies ...................... [OK in 9.36s]
22:45:24  22 of 43 START sql table model demo_silver.accounts ............................ [RUN]
25/01/24 22:45:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:45:47  22 of 43 OK created sql table model demo_silver.accounts ....................... [OK in 23.65s]
22:45:47  23 of 43 START sql table model demo_silver.customers ........................... [RUN]
25/01/24 22:45:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:46:08  23 of 43 OK created sql table model demo_silver.customers ...................... [OK in 19.59s]
22:46:08  24 of 43 START sql table model demo_silver.trades_history ...................... [RUN]
25/01/24 22:46:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:54:36  24 of 43 OK created sql table model demo_silver.trades_history ................. [OK in 508.05s]
22:54:36  25 of 43 START sql table model demo_gold.dim_broker ............................ [RUN]
25/01/24 22:54:36 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:54:42  25 of 43 OK created sql table model demo_gold.dim_broker ....................... [OK in 6.57s]
22:54:42  26 of 43 START sql table model demo_gold.dim_date .............................. [RUN]
25/01/24 22:54:43 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:54:45  26 of 43 OK created sql table model demo_gold.dim_date ......................... [OK in 2.72s]
22:54:45  27 of 43 START sql table model demo_gold.dim_company ........................... [RUN]
25/01/24 22:54:45 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:54:50  27 of 43 OK created sql table model demo_gold.dim_company ...................... [OK in 4.67s]
22:54:50  28 of 43 START sql table model demo_silver.financials .......................... [RUN]
25/01/24 22:54:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:56:16  28 of 43 OK created sql table model demo_silver.financials ..................... [OK in 85.95s]
22:56:16  29 of 43 START sql table model demo_silver.securities .......................... [RUN]
25/01/24 22:56:16 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:56:24  29 of 43 OK created sql table model demo_silver.securities ..................... [OK in 8.46s]
22:56:24  30 of 43 START sql table model demo_silver.cash_transactions ................... [RUN]
25/01/24 22:56:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:57:31  30 of 43 OK created sql table model demo_silver.cash_transactions .............. [OK in 66.54s]
22:57:31  31 of 43 START sql table model demo_gold.dim_customer .......................... [RUN]
25/01/24 22:57:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
22:57:58  31 of 43 OK created sql table model demo_gold.dim_customer ..................... [OK in 27.42s]
22:57:58  32 of 43 START sql table model demo_gold.dim_trade ............................. [RUN]
25/01/24 22:57:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:02:57  32 of 43 OK created sql table model demo_gold.dim_trade ........................ [OK in 298.70s]
23:02:57  33 of 43 START sql table model demo_silver.trades .............................. [RUN]
25/01/24 23:02:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:09:17  33 of 43 OK created sql table model demo_silver.trades ......................... [OK in 380.14s]
23:09:17  34 of 43 START sql table model demo_gold.dim_security .......................... [RUN]
25/01/24 23:09:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:09:23  34 of 43 OK created sql table model demo_gold.dim_security ..................... [OK in 6.20s]
23:09:23  35 of 43 START sql table model demo_silver.watches_history ..................... [RUN]
25/01/24 23:09:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:12:11  35 of 43 OK created sql table model demo_silver.watches_history ................ [OK in 168.06s]
23:12:11  36 of 43 START sql table model demo_gold.dim_account ........................... [RUN]
25/01/24 23:12:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:12:28  36 of 43 OK created sql table model demo_gold.dim_account ...................... [OK in 16.21s]
23:12:28  37 of 43 START sql table model demo_silver.holdings_history .................... [RUN]
25/01/24 23:12:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:14:45  37 of 43 OK created sql table model demo_silver.holdings_history ............... [OK in 137.12s]
23:14:45  38 of 43 START sql table model demo_silver.watches ............................. [RUN]
25/01/24 23:14:45 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:18:48  38 of 43 OK created sql table model demo_silver.watches ........................ [OK in 242.91s]
23:18:48  39 of 43 START sql table model demo_gold.fact_cash_transactions ................ [RUN]
25/01/24 23:18:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:20:03  39 of 43 OK created sql table model demo_gold.fact_cash_transactions ........... [OK in 75.07s]
23:20:03  40 of 43 START sql table model demo_gold.fact_trade ............................ [RUN]
25/01/24 23:20:03 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:24:39  40 of 43 OK created sql table model demo_gold.fact_trade ....................... [OK in 276.10s]
23:24:39  41 of 43 START sql table model demo_gold.fact_holdings ......................... [RUN]
25/01/24 23:24:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
25/01/24 23:33:41 ERROR YarnScheduler: Lost executor 2 on tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal: Container from a bad node: container_1737745789464_0013_01_000004 on host: tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal. Exit status: 143. Diagnostics: [2025-01-24 23:33:40.327]Container killed on request. Exit code is 143
[2025-01-24 23:33:40.329]Container exited with a non-zero exit code 143. 
[2025-01-24 23:33:40.329]Killed by external signal
.
25/01/24 23:33:41 WARN TaskSetManager: Lost task 2.0 in stage 153.0 (TID 1387) (tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1737745789464_0013_01_000004 on host: tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal. Exit status: 143. Diagnostics: [2025-01-24 23:33:40.327]Container killed on request. Exit code is 143
[2025-01-24 23:33:40.329]Container exited with a non-zero exit code 143. 
[2025-01-24 23:33:40.329]Killed by external signal
.
25/01/24 23:33:41 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1737745789464_0013_01_000004 on host: tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal. Exit status: 143. Diagnostics: [2025-01-24 23:33:40.327]Container killed on request. Exit code is 143
[2025-01-24 23:33:40.329]Container exited with a non-zero exit code 143. 
[2025-01-24 23:33:40.329]Killed by external signal
.
25/01/24 23:34:00 WARN TaskSetManager: Lost task 2.1 in stage 153.0 (TID 1388) (tbd-cluster-w-0.c.tbd-2024z-336369.internal executor 1): FetchFailed(BlockManagerId(2, tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal, 34891, None), shuffleId=36, mapIndex=34, mapId=1369, reduceId=14, message=
org.apache.spark.shuffle.FetchFailedException
	at org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1183)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:918)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage14.smj_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage14.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:323)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 2), which maintains the block data to fetch is dead.
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1150)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1142)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:702)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:192)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:240)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	... 8 more

)
25/01/24 23:34:00 WARN TaskSetManager: Lost task 3.0 in stage 153.0 (TID 1389) (tbd-cluster-w-0.c.tbd-2024z-336369.internal executor 1): FetchFailed(BlockManagerId(2, tbd-cluster-sw-6vw3.c.tbd-2024z-336369.internal, 34891, None), shuffleId=36, mapIndex=0, mapId=1335, reduceId=15, message=
org.apache.spark.shuffle.FetchFailedException
	at org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1183)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:918)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage14.smj_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage14.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:323)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 2), which maintains the block data to fetch is dead.
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1150)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1142)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:702)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:192)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:240)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	... 8 more

)
23:53:05  41 of 43 OK created sql table model demo_gold.fact_holdings .................... [OK in 1706.31s]
23:53:05  42 of 43 START sql table model demo_gold.fact_watches .......................... [RUN]
25/01/24 23:53:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
23:57:33  42 of 43 OK created sql table model demo_gold.fact_watches ..................... [OK in 268.37s]
23:57:33  43 of 43 START sql table model demo_gold.fact_cash_balances .................... [RUN]
25/01/24 23:57:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
00:04:09  43 of 43 OK created sql table model demo_gold.fact_cash_balances ............... [OK in 395.10s]
00:04:09  
00:04:09  Finished running 43 table models in 2 hours 4 minutes and 19.41 seconds (7459.41s).
00:04:09  
00:04:09  Completed successfully
00:04:09  
00:04:09  Done. PASS=43 WARN=0 ERROR=0 SKIP=0 TOTAL=43

Task exception was never retrieved
future: <Task finished name='Task-39' coro=<ScriptMagics.shebang.<locals>._handle_stream() done, defined at /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:211> exception=ValueError('Separator is not found, and chunk exceed the limit')>
Traceback (most recent call last):
  File "/usr/lib/python3.8/asyncio/streams.py", line 540, in readline
    line = await self.readuntil(sep)
  File "/usr/lib/python3.8/asyncio/streams.py", line 618, in readuntil
    raise exceptions.LimitOverrunError(
asyncio.exceptions.LimitOverrunError: Separator is not found, and chunk exceed the limit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py", line 213, in _handle_stream
    line = (await stream.readline()).decode("utf8", errors="replace")
  File "/usr/lib/python3.8/asyncio/streams.py", line 549, in readline
    raise ValueError(e.args[0])
ValueError: Separator is not found, and chunk exceed the limit
